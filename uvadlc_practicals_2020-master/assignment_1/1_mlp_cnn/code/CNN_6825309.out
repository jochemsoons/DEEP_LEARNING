learning_rate : 0.0001
max_steps : 5000
batch_size : 32
eval_freq : 10
data_dir : ./cifar10/cifar-10-batches-py
Device cuda
tensor(7.8828, device='cuda:0', grad_fn=<NllLossBackward>)
[W TensorIterator.cpp:918] Warning: Mixed memory format inputs detected while calling the operator. The operator will output contiguous tensor even if some of the inputs are in channels_last format. (function operator())
Traceback (most recent call last):
  File "train_convnet_pytorch.py", line 200, in <module>
    main()
  File "train_convnet_pytorch.py", line 182, in main
    train()
  File "train_convnet_pytorch.py", line 143, in train
    predictions = CNN.forward(x_test)
  File "/home/lgpu0277/practical_1/convnet_pytorch.py", line 91, in forward
    out = out + self.preact2(out)
  File "/home/lgpu0277/.conda/envs/dl2020/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0277/.conda/envs/dl2020/lib/python3.7/site-packages/torch/nn/modules/container.py", line 117, in forward
    input = module(input)
  File "/home/lgpu0277/.conda/envs/dl2020/lib/python3.7/site-packages/torch/nn/modules/module.py", line 722, in _call_impl
    result = self.forward(*input, **kwargs)
  File "/home/lgpu0277/.conda/envs/dl2020/lib/python3.7/site-packages/torch/nn/modules/activation.py", line 102, in forward
    return F.relu(input, inplace=self.inplace)
  File "/home/lgpu0277/.conda/envs/dl2020/lib/python3.7/site-packages/torch/nn/functional.py", line 1119, in relu
    result = torch.relu(input)
RuntimeError: CUDA out of memory. Tried to allocate 20.00 MiB (GPU 0; 10.92 GiB total capacity; 10.31 GiB already allocated; 15.44 MiB free; 10.32 GiB reserved in total by PyTorch)
srun: error: r30n6: task 0: Exited with exit code 1
